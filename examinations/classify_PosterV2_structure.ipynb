{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline에 넣기 위해 PosterV2 코드 구조를 정리합니다.\n",
        "- 현재 (대략적인) 코드 구조 는 아래와 같습니다."
      ],
      "metadata": {
        "id": "5IqrgXyEBywU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "```\n",
        "est_wassup_03/\n",
        "\tconfigs/  # yolo yaml 파일\n",
        "\tcore\n",
        "\t\tconfigs  # config class\n",
        "\t\tdatasets  # custom dataset class\n",
        "\t\tmodels\n",
        "\t\tutils\n",
        "\ttrain_tools  # 실제 돌아갈 script\n",
        "\ttools  # data_tools로 변경 예정, data preprocessing관련 script\n",
        "\tinference  # infer_tools로 변경 예정,  inference관련 script\n",
        "\tutils\n",
        "\tdata # 원본 데이터(서버에서 원하는 비율로 sampling 된 )\n",
        "\tfeatures  # 원본 데이터에서 구조, split, crop등 어떻게든 전처리된 것\n",
        "\tresults  # train, eval, infer의 결과물 저장\n",
        "\t\tdetect\n",
        "\t\t\ttrain\n",
        "\t\t\teval\n",
        "\t\t\tinfer\n",
        "\t\tclassify\n",
        "\t\t2d_gen\n",
        "\t\t3d_gen\n",
        "```"
      ],
      "metadata": {
        "id": "78YbF9TqB6dM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "아래와 같이 넣어보자"
      ],
      "metadata": {
        "id": "X3TyPk7tCVim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "```\n",
        "est_wassup_03/\n",
        "\t*checkpoint/ # 데이터셋으로 학습시킨 best weights 저장\n",
        "    *affectnet-7-model_best.pth\n",
        "    *caer-s-model_best.pth\n",
        "    *raf-db-model_best.pth\n",
        "  configs/\n",
        "\tcore/\n",
        "\t\tconfigs/\n",
        "\t\tdatasets/\n",
        "\t\tmodels/ # 의견: detect_models/, cls_models/, 2d_gen_models/, 3d_gen_models/ 식으로 나누는 건 어떨까\n",
        "      *pretrain/ # pretrained 모델 pth 저장\n",
        "          *ir50.pth\n",
        "          *mobilefacenet_model_best_pth.tar\n",
        "      *ir50.py\n",
        "      *matrix.py\n",
        "      *mobilefacenet.py\n",
        "      *PosterV2_7cls.py\n",
        "      *vit_model.py\n",
        "\t\tutils/\n",
        "\ttrain_tools/\n",
        "\ttools/  # (data_tools)\n",
        "    *sam.py # poster data preprocessing (모하는 친구인지 자세히 봐야함)\n",
        "\tinference/\n",
        "\tutils/\n",
        "\tdata/\n",
        "\tfeatures/\n",
        "\tresults/ # log -> results로 poster 메인 코드쪽 변경\n",
        "\t\tdetect/\n",
        "\t\t\ttrain/\n",
        "\t\t\teval/\n",
        "\t\t\tinfer/\n",
        "\t\tcls/\n",
        "\t\t2d_gen/\n",
        "\t\t3d_gen/\n",
        "  *main_poster_cls.py  # poster 메인 함수 (7 classes)\n",
        "```"
      ],
      "metadata": {
        "id": "W6sYtkMfCXXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 정리시작\n",
        "1. main.py  \n",
        "2. models\n",
        "  - pretrain/ : 여기에 pretrained 모델 넣습니다.\n",
        "  - ir50.py\n",
        "  - matrix.py: confusion matrix plot그리는거라 다른데로 옮기는게 좋겠죠?\n",
        "  - mobilefacenet.py\n",
        "  - PosterV2_7cls.py\n",
        "  - vit_model.py\n",
        "3. tools/\n",
        "  - sam.py\n",
        "4. checkpoint/ : 여기에 데이터셋 best weights 넣습니다.\n",
        "\n",
        "\n",
        "[pretrained model, dataset weights]는 공유 드라이브 폴더에 넣어두었습니다.\n",
        "https://drive.google.com/drive/u/2/folders/1NvgYRhrzxP4E-aR-QbTIZIO_60zGwVNu\n",
        "\n",
        "----------------------------------------------------------"
      ],
      "metadata": {
        "id": "nFBuIx7_LcT7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### main_poster_cls.py\n",
        "- 일부 라이브러리 변경 (현재 지원되지 않는 버전 문제로 인함)\n",
        "- log 저장 위치 변경: `./log/` -> `./resutls/cls/`\n",
        "\n",
        "[arg parser 정리]  \n",
        "1. --data : 데이터 소스 경로\n",
        "2. --data_type: 데이터셋마다 augment방법이 조금씩 다름. 일단 우리는 CAER-S와 동일한 걸로 테스트한다고 생각함.\n",
        "3. --checkpoint_path: 데이터셋마다 학습한 weights 저장 경로\n",
        "4. --best_checkpoint_path: 3번의 best weights\n",
        "5. -j, --workers: dataloader에 사용하는 worker 개수\n",
        "6. --epochs\n",
        "7. --start-epoch: resume용 manual start epoch\n",
        "8. -b, --batch-size\n",
        "9. --optimizer: adamw, adam, sgd\n",
        "10. --lr\n",
        "11. --momentum\n",
        "12. --wd, --weight-decay\n",
        "13. -p, --print-freq\n",
        "14. -e, --evaluate: 테스트 셋에다가 evaluate할때 쓰는 모드\n",
        "15. --beta: argparser에만 있고 main함수에선 사용 안됨\n",
        "16. --gpu"
      ],
      "metadata": {
        "id": "k_olJr-YL9uP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import warnings\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import torch.utils.data as data\n",
        "import os\n",
        "import argparse\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "from tools.sam import SAM # 원래 data_preprocessing 폴더에서 import해오던걸 수정함\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torch.utils.data.distributed\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import datetime\n",
        "from torchsampler import ImbalancedDatasetSampler\n",
        "from models.PosterV2_7cls import *\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "now = datetime.datetime.now()\n",
        "time_str = now.strftime(\"[%m-%d]-[%H-%M]-\")\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--data', type=str, default=r'/home/Dataset/RAF')\n",
        "parser.add_argument('--data_type', default='KFE-DB', choices=['RAF-DB', 'AffectNet-7', 'CAER-S', 'KFE-DB'], # 'KFE-DB' our dataset\n",
        "                        type=str, help='dataset option')\n",
        "parser.add_argument('--checkpoint_path', type=str, default='./checkpoint/' + time_str + 'model.pth')\n",
        "parser.add_argument('--best_checkpoint_path', type=str, default='./checkpoint/' + time_str + 'model_best.pth')\n",
        "parser.add_argument('-j', '--workers', default=4, type=int, metavar='N', help='number of data loading workers')\n",
        "parser.add_argument('--epochs', default=200, type=int, metavar='N', help='number of total epochs to run')\n",
        "parser.add_argument('--start-epoch', default=0, type=int, metavar='N', help='manual epoch number (useful on restarts)')\n",
        "parser.add_argument('-b', '--batch-size', default=144, type=int, metavar='N')\n",
        "parser.add_argument('--optimizer', type=str, default=\"adam\", help='Optimizer, adam or sgd.')\n",
        "\n",
        "parser.add_argument('--lr', '--learning-rate', default=0.000035, type=float, metavar='LR', dest='lr')\n",
        "parser.add_argument('--momentum', default=0.9, type=float, metavar='M')\n",
        "parser.add_argument('--wd', '--weight-decay', default=1e-4, type=float, metavar='W', dest='weight_decay')\n",
        "parser.add_argument('-p', '--print-freq', default=30, type=int, metavar='N', help='print frequency')\n",
        "parser.add_argument('--resume', default=None, type=str, metavar='PATH', help='path to checkpoint')\n",
        "parser.add_argument('-e', '--evaluate', default=None, type=str, help='evaluate model on test set')\n",
        "parser.add_argument('--beta', type=float, default=0.6)\n",
        "parser.add_argument('--gpu', type=str, default='0')\n",
        "args = parser.parse_args()\n",
        "\n",
        "\n",
        "def main():\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
        "    best_acc = 0\n",
        "    print('Training time: ' + now.strftime(\"%m-%d %H:%M\"))\n",
        "\n",
        "    # create model\n",
        "    model = pyramid_trans_expr2(img_size=224, num_classes=7)\n",
        "\n",
        "    model = torch.nn.DataParallel(model).cuda()\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    if args.optimizer == 'adamw':\n",
        "        base_optimizer = torch.optim.AdamW\n",
        "    elif args.optimizer == 'adam':\n",
        "        base_optimizer = torch.optim.Adam\n",
        "    elif args.optimizer == 'sgd':\n",
        "        base_optimizer = torch.optim.SGD\n",
        "    else:\n",
        "        raise ValueError(\"Optimizer not supported.\")\n",
        "\n",
        "    optimizer = SAM(model.parameters(), base_optimizer, lr=args.lr, rho=0.05, adaptive=False, )\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)\n",
        "    recorder = RecorderMeter(args.epochs)\n",
        "    recorder1 = RecorderMeter1(args.epochs)\n",
        "\n",
        "    if args.resume:\n",
        "        if os.path.isfile(args.resume):\n",
        "            print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
        "            checkpoint = torch.load(args.resume)\n",
        "            args.start_epoch = checkpoint['epoch']\n",
        "            best_acc = checkpoint['best_acc']\n",
        "            recorder = checkpoint['recorder']\n",
        "            recorder1 = checkpoint['recorder1']\n",
        "            best_acc = best_acc.to()\n",
        "            model.load_state_dict(checkpoint['state_dict'])\n",
        "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "            print(\"=> loaded checkpoint '{}' (epoch {})\".format(args.resume, checkpoint['epoch']))\n",
        "        else:\n",
        "            print(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "    # Data loading code\n",
        "    traindir = os.path.join(args.data, 'train')\n",
        "\n",
        "    valdir = os.path.join(args.data, 'test')\n",
        "\n",
        "    if args.evaluate is None:\n",
        "\n",
        "        if args.data_type == 'RAF-DB':\n",
        "            train_dataset = datasets.ImageFolder(traindir,\n",
        "                                                 transforms.Compose([transforms.Resize((224, 224)),\n",
        "                                                                     transforms.RandomHorizontalFlip(),\n",
        "                                                                     transforms.ToTensor(),\n",
        "                                                                     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                                                          std=[0.229, 0.224, 0.225]),\n",
        "                                                                     transforms.RandomErasing(scale=(0.02, 0.1))]))\n",
        "        else: # our dataset\n",
        "            train_dataset = datasets.ImageFolder(traindir,\n",
        "                                                 transforms.Compose([transforms.Resize((224, 224)),\n",
        "                                                                     transforms.RandomHorizontalFlip(),\n",
        "                                                                     transforms.ToTensor(),\n",
        "                                                                     transforms.Normalize(mean=[0.485, 0.456, 0.406],  # normalization\n",
        "                                                                                          std=[0.229, 0.224, 0.225]),\n",
        "                                                                     transforms.RandomErasing(p=1, scale=(0.05, 0.05))]))\n",
        "\n",
        "        if args.data_type == 'AffectNet-7':\n",
        "            train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                                       sampler=ImbalancedDatasetSampler(train_dataset),\n",
        "                                                       batch_size=args.batch_size,\n",
        "                                                       shuffle=False,\n",
        "                                                       num_workers=args.workers,\n",
        "                                                       pin_memory=True)\n",
        "\n",
        "        else: # our dataset\n",
        "            train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                                       batch_size=args.batch_size,\n",
        "                                                       shuffle=True,\n",
        "                                                       num_workers=args.workers,\n",
        "                                                       pin_memory=True)\n",
        "\n",
        "    test_dataset = datasets.ImageFolder(valdir,\n",
        "                                        transforms.Compose([transforms.Resize((224, 224)),\n",
        "                                                            transforms.ToTensor(),\n",
        "                                                            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                                                 std=[0.229, 0.224, 0.225]),\n",
        "                                                            ]))\n",
        "\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(test_dataset,\n",
        "                                             batch_size=args.batch_size,\n",
        "                                             shuffle=False,\n",
        "                                             num_workers=args.workers,\n",
        "                                             pin_memory=True)\n",
        "\n",
        "    if args.evaluate is not None:\n",
        "        if os.path.isfile(args.evaluate):\n",
        "            print(\"=> loading checkpoint '{}'\".format(args.evaluate))\n",
        "            checkpoint = torch.load(args.evaluate)\n",
        "            best_acc = checkpoint['best_acc']\n",
        "            best_acc = best_acc.to()\n",
        "            print(f'best_acc:{best_acc}')\n",
        "            model.load_state_dict(checkpoint['state_dict'])\n",
        "            print(\"=> loaded checkpoint '{}' (epoch {})\".format(args.evaluate, checkpoint['epoch']))\n",
        "        else:\n",
        "            print(\"=> no checkpoint found at '{}'\".format(args.evaluate))\n",
        "        validate(val_loader, model, criterion, args)\n",
        "        return\n",
        "\n",
        "    matrix = None\n",
        "\n",
        "    for epoch in range(args.start_epoch, args.epochs):\n",
        "\n",
        "        current_learning_rate = optimizer.state_dict()['param_groups'][0]['lr']\n",
        "        print('Current learning rate: ', current_learning_rate)\n",
        "        txt_name = './results/cls/' + time_str + 'log.txt'\n",
        "        with open(txt_name, 'a') as f:\n",
        "            f.write('Current learning rate: ' + str(current_learning_rate) + '\\n')\n",
        "\n",
        "        # train for one epoch\n",
        "        train_acc, train_los = train(train_loader, model, criterion, optimizer, epoch, args)\n",
        "\n",
        "        # evaluate on validation set\n",
        "        val_acc, val_los, output, target, D = validate(val_loader, model, criterion, args)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        recorder.update(epoch, train_los, train_acc, val_los, val_acc)\n",
        "        recorder1.update(output, target)\n",
        "\n",
        "        curve_name = time_str + 'cnn.png'\n",
        "        recorder.plot_curve(os.path.join('./results/cls/', curve_name))\n",
        "\n",
        "        # remember best acc and save checkpoint\n",
        "        is_best = val_acc > best_acc\n",
        "        best_acc = max(val_acc, best_acc)\n",
        "\n",
        "        print('Current best accuracy: ', best_acc.item())\n",
        "\n",
        "        if is_best:\n",
        "            matrix = D\n",
        "\n",
        "        print('Current best matrix: ', matrix)\n",
        "\n",
        "        txt_name = './results/cls' + time_str + 'log.txt'\n",
        "        with open(txt_name, 'a') as f:\n",
        "            f.write('Current best accuracy: ' + str(best_acc.item()) + '\\n')\n",
        "\n",
        "        save_checkpoint({'epoch': epoch + 1,\n",
        "                         'state_dict': model.state_dict(),\n",
        "                         'best_acc': best_acc,\n",
        "                         'optimizer': optimizer.state_dict(),\n",
        "                         'recorder1': recorder1,\n",
        "                         'recorder': recorder}, is_best, args)\n",
        "\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, epoch, args):\n",
        "    losses = AverageMeter('Loss', ':.4f')\n",
        "    top1 = AverageMeter('Accuracy', ':6.3f')\n",
        "    progress = ProgressMeter(len(train_loader),\n",
        "                             [losses, top1],\n",
        "                             prefix=\"Epoch: [{}]\".format(epoch))\n",
        "\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    for i, (images, target) in enumerate(train_loader):\n",
        "        # print(images.shape)\n",
        "        images = images.cuda()\n",
        "        target = target.cuda()\n",
        "\n",
        "        # compute output\n",
        "        output = model(images)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        acc1, _ = accuracy(output, target, topk=(1, 5))\n",
        "        losses.update(loss.item(), images.size(0))\n",
        "        top1.update(acc1[0], images.size(0))\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # optimizer.step()\n",
        "        optimizer.first_step(zero_grad=True)\n",
        "        images = images.cuda()\n",
        "        target = target.cuda()\n",
        "\n",
        "        # compute output\n",
        "        output = model(images)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        acc1, _ = accuracy(output, target, topk=(1, 5))\n",
        "        losses.update(loss.item(), images.size(0))\n",
        "        top1.update(acc1[0], images.size(0))\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.second_step(zero_grad=True)\n",
        "\n",
        "        # print loss and accuracy\n",
        "        if i % args.print_freq == 0:\n",
        "            progress.display(i)\n",
        "\n",
        "    return top1.avg, losses.avg\n",
        "\n",
        "\n",
        "def validate(val_loader, model, criterion, args):\n",
        "    losses = AverageMeter('Loss', ':.4f')\n",
        "    top1 = AverageMeter('Accuracy', ':6.3f')\n",
        "    progress = ProgressMeter(len(val_loader),\n",
        "                             [losses, top1],\n",
        "                             prefix='Test: ')\n",
        "\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "    D = [[0, 0, 0, 0, 0, 0, 0],\n",
        "         [0, 0, 0, 0, 0, 0, 0],\n",
        "         [0, 0, 0, 0, 0, 0, 0],\n",
        "         [0, 0, 0, 0, 0, 0, 0],\n",
        "         [0, 0, 0, 0, 0, 0, 0],\n",
        "         [0, 0, 0, 0, 0, 0, 0],\n",
        "         [0, 0, 0, 0, 0, 0, 0]]\n",
        "    with torch.no_grad():\n",
        "        for i, (images, target) in enumerate(val_loader):\n",
        "            images = images.cuda()\n",
        "            target = target.cuda()\n",
        "            output = model(images)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            acc, _ = accuracy(output, target, topk=(1, 5))\n",
        "            losses.update(loss.item(), images.size(0))\n",
        "            top1.update(acc[0], images.size(0))\n",
        "\n",
        "            topk = (1,)\n",
        "            # \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "            with torch.no_grad():\n",
        "                maxk = max(topk)\n",
        "                # batch_size = target.size(0)\n",
        "                _, pred = output.topk(maxk, 1, True, True)\n",
        "                pred = pred.t()\n",
        "\n",
        "            output = pred\n",
        "            target = target.squeeze().cpu().numpy()\n",
        "            output = output.squeeze().cpu().numpy()\n",
        "\n",
        "            im_re_label = np.array(target)\n",
        "            im_pre_label = np.array(output)\n",
        "            y_ture = im_re_label.flatten()\n",
        "            im_re_label.transpose()\n",
        "            y_pred = im_pre_label.flatten()\n",
        "            im_pre_label.transpose()\n",
        "\n",
        "            C = metrics.confusion_matrix(y_ture, y_pred, labels=[0, 1, 2, 3, 4, 5, 6])\n",
        "            D += C\n",
        "\n",
        "            if i % args.print_freq == 0:\n",
        "                progress.display(i)\n",
        "\n",
        "        print(' **** Accuracy {top1.avg:.3f} *** '.format(top1=top1))\n",
        "        with open('./results/cls/' + time_str + 'log.txt', 'a') as f:\n",
        "            f.write(' * Accuracy {top1.avg:.3f}'.format(top1=top1) + '\\n')\n",
        "    print(D)\n",
        "    return top1.avg, losses.avg, output, target, D\n",
        "\n",
        "def save_checkpoint(state, is_best, args):\n",
        "    torch.save(state, args.checkpoint_path)\n",
        "    if is_best:\n",
        "        best_state = state.pop('optimizer')\n",
        "        torch.save(best_state, args.best_checkpoint_path)\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "\n",
        "class ProgressMeter(object):\n",
        "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
        "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
        "        self.meters = meters\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def display(self, batch):\n",
        "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
        "        entries += [str(meter) for meter in self.meters]\n",
        "        print_txt = '\\t'.join(entries)\n",
        "        print(print_txt)\n",
        "        txt_name = './results/cls/' + time_str + 'log.txt'\n",
        "        with open(txt_name, 'a') as f:\n",
        "            f.write(print_txt + '\\n')\n",
        "\n",
        "    def _get_batch_fmtstr(self, num_batches):\n",
        "        num_digits = len(str(num_batches // 1))\n",
        "        fmt = '{:' + str(num_digits) + 'd}'\n",
        "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].contiguous().view(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "\n",
        "\n",
        "labels = ['A', 'B', 'C', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O']  #TODO 폴더명 기준으로 변경?\n",
        "\n",
        "\n",
        "class RecorderMeter1(object):\n",
        "    \"\"\"Computes and stores the minimum loss value and its epoch index\"\"\"\n",
        "\n",
        "    def __init__(self, total_epoch):\n",
        "        self.reset(total_epoch)\n",
        "\n",
        "    def reset(self, total_epoch):\n",
        "        self.total_epoch = total_epoch\n",
        "        self.current_epoch = 0\n",
        "        self.epoch_losses = np.zeros((self.total_epoch, 2), dtype=np.float32)  # [epoch, train/val]\n",
        "        self.epoch_accuracy = np.zeros((self.total_epoch, 2), dtype=np.float32)  # [epoch, train/val]\n",
        "\n",
        "    def update(self, output, target):\n",
        "        self.y_pred = output\n",
        "        self.y_true = target\n",
        "\n",
        "    def plot_confusion_matrix(self, cm, title='Confusion Matrix', cmap=plt.cm.binary):\n",
        "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "        y_true = self.y_true\n",
        "        y_pred = self.y_pred\n",
        "\n",
        "        plt.title(title)\n",
        "        plt.colorbar()\n",
        "        xlocations = np.array(range(len(labels)))\n",
        "        plt.xticks(xlocations, labels, rotation=90)\n",
        "        plt.yticks(xlocations, labels)\n",
        "        plt.ylabel('True label')\n",
        "        plt.xlabel('Predicted label')\n",
        "\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        np.set_printoptions(precision=2)\n",
        "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        plt.figure(figsize=(12, 8), dpi=120)\n",
        "\n",
        "        ind_array = np.arange(len(labels))\n",
        "        x, y = np.meshgrid(ind_array, ind_array)\n",
        "        for x_val, y_val in zip(x.flatten(), y.flatten()):\n",
        "            c = cm_normalized[y_val][x_val]\n",
        "            if c > 0.01:\n",
        "                plt.text(x_val, y_val, \"%0.2f\" % (c,), color='red', fontsize=7, va='center', ha='center')\n",
        "        # offset the tick\n",
        "        tick_marks = np.arange(len(7))\n",
        "        plt.gca().set_xticks(tick_marks, minor=True)\n",
        "        plt.gca().set_yticks(tick_marks, minor=True)\n",
        "        plt.gca().xaxis.set_ticks_position('none')\n",
        "        plt.gca().yaxis.set_ticks_position('none')\n",
        "        plt.grid(True, which='minor', linestyle='-')\n",
        "        plt.gcf().subplots_adjust(bottom=0.15)\n",
        "\n",
        "        ConfusionMatrixDisplay(cm_normalized, title='Normalized confusion matrix')\n",
        "        # show confusion matrix\n",
        "        plt.savefig('./results/cls/confusion_matrix.png', format='png')\n",
        "        # fig.savefig(save_path, dpi=dpi, bbox_inches='tight')\n",
        "        print('Saved figure')\n",
        "        plt.show()\n",
        "\n",
        "    def matrix(self):\n",
        "        target = self.y_true\n",
        "        output = self.y_pred\n",
        "        im_re_label = np.array(target)\n",
        "        im_pre_label = np.array(output)\n",
        "        y_ture = im_re_label.flatten()\n",
        "        # im_re_label.transpose()\n",
        "        y_pred = im_pre_label.flatten()\n",
        "        im_pre_label.transpose()\n",
        "\n",
        "class RecorderMeter(object):\n",
        "    \"\"\"Computes and stores the minimum loss value and its epoch index\"\"\"\n",
        "\n",
        "    def __init__(self, total_epoch):\n",
        "        self.reset(total_epoch)\n",
        "\n",
        "    def reset(self, total_epoch):\n",
        "        self.total_epoch = total_epoch\n",
        "        self.current_epoch = 0\n",
        "        self.epoch_losses = np.zeros((self.total_epoch, 2), dtype=np.float32)  # [epoch, train/val]\n",
        "        self.epoch_accuracy = np.zeros((self.total_epoch, 2), dtype=np.float32)  # [epoch, train/val]\n",
        "\n",
        "    def update(self, idx, train_loss, train_acc, val_loss, val_acc):\n",
        "        self.epoch_losses[idx, 0] = train_loss * 30\n",
        "        self.epoch_losses[idx, 1] = val_loss * 30\n",
        "        self.epoch_accuracy[idx, 0] = train_acc\n",
        "        self.epoch_accuracy[idx, 1] = val_acc\n",
        "        self.current_epoch = idx + 1\n",
        "\n",
        "    def plot_curve(self, save_path):\n",
        "        title = 'the accuracy/loss curve of train/val'\n",
        "        dpi = 80\n",
        "        width, height = 1800, 800\n",
        "        legend_fontsize = 10\n",
        "        figsize = width / float(dpi), height / float(dpi)\n",
        "\n",
        "        fig = plt.figure(figsize=figsize)\n",
        "        x_axis = np.array([i for i in range(self.total_epoch)])  # epochs\n",
        "        y_axis = np.zeros(self.total_epoch)\n",
        "\n",
        "        plt.xlim(0, self.total_epoch)\n",
        "        plt.ylim(0, 100)\n",
        "        interval_y = 5\n",
        "        interval_x = 5\n",
        "        plt.xticks(np.arange(0, self.total_epoch + interval_x, interval_x))\n",
        "        plt.yticks(np.arange(0, 100 + interval_y, interval_y))\n",
        "        plt.grid()\n",
        "        plt.title(title, fontsize=20)\n",
        "        plt.xlabel('the training epoch', fontsize=16)\n",
        "        plt.ylabel('accuracy', fontsize=16)\n",
        "\n",
        "        y_axis[:] = self.epoch_accuracy[:, 0]\n",
        "        plt.plot(x_axis, y_axis, color='g', linestyle='-', label='train-accuracy', lw=2)\n",
        "        plt.legend(loc=4, fontsize=legend_fontsize)\n",
        "\n",
        "        y_axis[:] = self.epoch_accuracy[:, 1]\n",
        "        plt.plot(x_axis, y_axis, color='y', linestyle='-', label='valid-accuracy', lw=2)\n",
        "        plt.legend(loc=4, fontsize=legend_fontsize)\n",
        "\n",
        "        y_axis[:] = self.epoch_losses[:, 0]\n",
        "        plt.plot(x_axis, y_axis, color='g', linestyle=':', label='train-loss-x30', lw=2)\n",
        "        plt.legend(loc=4, fontsize=legend_fontsize)\n",
        "\n",
        "        y_axis[:] = self.epoch_losses[:, 1]\n",
        "        plt.plot(x_axis, y_axis, color='y', linestyle=':', label='valid-loss-x30', lw=2)\n",
        "        plt.legend(loc=4, fontsize=legend_fontsize)\n",
        "\n",
        "        if save_path is not None:\n",
        "            fig.savefig(save_path, dpi=dpi, bbox_inches='tight')\n",
        "            print('Saved figure')\n",
        "        plt.close(fig)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "2NA9dytCLdfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### models/ir50.py\n"
      ],
      "metadata": {
        "id": "bJq8F0XUsbGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import Linear, Conv2d, BatchNorm1d, BatchNorm2d, PReLU, ReLU, Sigmoid, Dropout2d, Dropout, AvgPool2d, \\\n",
        "    MaxPool2d, AdaptiveAvgPool2d, Sequential, Module, Parameter\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from collections import namedtuple\n",
        "import math\n",
        "import pdb\n",
        "\n",
        "\n",
        "##################################  Original Arcface Model #############################################################\n",
        "\n",
        "class Flatten(Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1)\n",
        "\n",
        "\n",
        "def l2_norm(input, axis=1):\n",
        "    norm = torch.norm(input, 2, axis, True)\n",
        "    output = torch.div(input, norm)\n",
        "    return output\n",
        "\n",
        "\n",
        "class SEModule(Module):\n",
        "    def __init__(self, channels, reduction):\n",
        "        super(SEModule, self).__init__()\n",
        "        self.avg_pool = AdaptiveAvgPool2d(1)\n",
        "        self.fc1 = Conv2d(\n",
        "            channels, channels // reduction, kernel_size=1, padding=0, bias=False)\n",
        "        self.relu = ReLU(inplace=True)\n",
        "        self.fc2 = Conv2d(\n",
        "            channels // reduction, channels, kernel_size=1, padding=0, bias=False)\n",
        "        self.sigmoid = Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        module_input = x\n",
        "        x = self.avg_pool(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return module_input * x\n",
        "\n",
        "\n",
        "# i = 0\n",
        "\n",
        "class bottleneck_IR(Module):\n",
        "    def __init__(self, in_channel, depth, stride):\n",
        "        super(bottleneck_IR, self).__init__()\n",
        "        if in_channel == depth:\n",
        "            self.shortcut_layer = MaxPool2d(1, stride)\n",
        "        else:\n",
        "            self.shortcut_layer = Sequential(\n",
        "                Conv2d(in_channel, depth, (1, 1), stride, bias=False), BatchNorm2d(depth))\n",
        "        self.res_layer = Sequential(\n",
        "            BatchNorm2d(in_channel),\n",
        "            Conv2d(in_channel, depth, (3, 3), (1, 1), 1, bias=False), PReLU(depth),\n",
        "            Conv2d(depth, depth, (3, 3), stride, 1, bias=False), BatchNorm2d(depth))\n",
        "        i = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = self.shortcut_layer(x)\n",
        "        # print(shortcut.shape)\n",
        "        # print('---s---')\n",
        "        res = self.res_layer(x)\n",
        "        # print(res.shape)\n",
        "        # print('---r---')\n",
        "        # i = i + 50\n",
        "        # print(i)\n",
        "        # print('50')\n",
        "        return res + shortcut\n",
        "\n",
        "\n",
        "class bottleneck_IR_SE(Module):\n",
        "    def __init__(self, in_channel, depth, stride):\n",
        "        super(bottleneck_IR_SE, self).__init__()\n",
        "        if in_channel == depth:\n",
        "            self.shortcut_layer = MaxPool2d(1, stride)\n",
        "        else:\n",
        "            self.shortcut_layer = Sequential(\n",
        "                Conv2d(in_channel, depth, (1, 1), stride, bias=False),\n",
        "                BatchNorm2d(depth))\n",
        "        self.res_layer = Sequential(\n",
        "            BatchNorm2d(in_channel),\n",
        "            Conv2d(in_channel, depth, (3, 3), (1, 1), 1, bias=False),\n",
        "            PReLU(depth),\n",
        "            Conv2d(depth, depth, (3, 3), stride, 1, bias=False),\n",
        "            BatchNorm2d(depth),\n",
        "            SEModule(depth, 16)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = self.shortcut_layer(x)\n",
        "        res = self.res_layer(x)\n",
        "        return res + shortcut\n",
        "\n",
        "\n",
        "class Bottleneck(namedtuple('Block', ['in_channel', 'depth', 'stride'])):\n",
        "    '''A named tuple describing a ResNet block.'''\n",
        "    # print('50')\n",
        "\n",
        "\n",
        "def get_block(in_channel, depth, num_units, stride=2):\n",
        "    return [Bottleneck(in_channel, depth, stride)] + [Bottleneck(depth, depth, 1) for i in range(num_units - 1)]\n",
        "\n",
        "\n",
        "def get_blocks(num_layers):\n",
        "    if num_layers == 50:\n",
        "        blocks1 = [\n",
        "            get_block(in_channel=64, depth=64, num_units=3),\n",
        "            # get_block(in_channel=64, depth=128, num_units=4),\n",
        "            # get_block(in_channel=128, depth=256, num_units=14),\n",
        "            # get_block(in_channel=256, depth=512, num_units=3)\n",
        "        ]\n",
        "        blocks2 = [\n",
        "            # get_block(in_channel=64, depth=64, num_units=3),\n",
        "            get_block(in_channel=64, depth=128, num_units=4),\n",
        "            # get_block(in_channel=128, depth=256, num_units=14),\n",
        "            # get_block(in_channel=256, depth=512, num_units=3)\n",
        "        ]\n",
        "        blocks3 = [\n",
        "            # get_block(in_channel=64, depth=64, num_units=3),\n",
        "            # get_block(in_channel=64, depth=128, num_units=4),\n",
        "            get_block(in_channel=128, depth=256, num_units=14),\n",
        "            # get_block(in_channel=256, depth=512, num_units=3)\n",
        "        ]\n",
        "\n",
        "    elif num_layers == 100:\n",
        "        blocks = [\n",
        "            get_block(in_channel=64, depth=64, num_units=3),\n",
        "            get_block(in_channel=64, depth=128, num_units=13),\n",
        "            get_block(in_channel=128, depth=256, num_units=30),\n",
        "            get_block(in_channel=256, depth=512, num_units=3)\n",
        "        ]\n",
        "    elif num_layers == 152:\n",
        "        blocks = [\n",
        "            get_block(in_channel=64, depth=64, num_units=3),\n",
        "            get_block(in_channel=64, depth=128, num_units=8),\n",
        "            get_block(in_channel=128, depth=256, num_units=36),\n",
        "            get_block(in_channel=256, depth=512, num_units=3)\n",
        "        ]\n",
        "    return blocks1, blocks2, blocks3\n",
        "\n",
        "\n",
        "class Backbone(Module):\n",
        "    def __init__(self, num_layers, drop_ratio, mode='ir'):\n",
        "        super(Backbone, self).__init__()\n",
        "        # assert num_layers in [50, 100, 152], 'num_layers should be 50,100, or 152'\n",
        "        assert mode in ['ir', 'ir_se'], 'mode should be ir or ir_se'\n",
        "        blocks1, blocks2, blocks3 = get_blocks(num_layers)\n",
        "        # blocks2 = get_blocks(num_layers)\n",
        "        if mode == 'ir':\n",
        "            unit_module = bottleneck_IR\n",
        "        elif mode == 'ir_se':\n",
        "            unit_module = bottleneck_IR_SE\n",
        "        self.input_layer = Sequential(Conv2d(3, 64, (3, 3), 1, 1, bias=False),\n",
        "                                      BatchNorm2d(64),\n",
        "                                      PReLU(64))\n",
        "        self.output_layer = Sequential(BatchNorm2d(512),\n",
        "                                       Dropout(drop_ratio),\n",
        "                                       Flatten(),\n",
        "                                       Linear(512 * 7 * 7, 512),\n",
        "                                       BatchNorm1d(512))\n",
        "        modules1 = []\n",
        "        for block in blocks1:\n",
        "            for bottleneck in block:\n",
        "                modules1.append(\n",
        "                    unit_module(bottleneck.in_channel,\n",
        "                                bottleneck.depth,\n",
        "                                bottleneck.stride))\n",
        "\n",
        "        modules2 = []\n",
        "        for block in blocks2:\n",
        "            for bottleneck in block:\n",
        "                modules2.append(\n",
        "                    unit_module(bottleneck.in_channel,\n",
        "                                bottleneck.depth,\n",
        "                                bottleneck.stride))\n",
        "\n",
        "        modules3 = []\n",
        "        for block in blocks3:\n",
        "            for bottleneck in block:\n",
        "                modules3.append(\n",
        "                    unit_module(bottleneck.in_channel,\n",
        "                                bottleneck.depth,\n",
        "                                bottleneck.stride))\n",
        "        # modules4 = []\n",
        "        # for block in blocks4:\n",
        "        #     for bottleneck in block:\n",
        "        #         modules4.append(\n",
        "        #             unit_module(bottleneck.in_channel,\n",
        "        #                         bottleneck.depth,\n",
        "        #                         bottleneck.stride))\n",
        "        self.body1 = Sequential(*modules1)\n",
        "        self.body2 = Sequential(*modules2)\n",
        "        self.body3 = Sequential(*modules3)\n",
        "        # self.body4 = Sequential(*modules4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.interpolate(x, size=112)\n",
        "        x = self.input_layer(x)\n",
        "        x1 = self.body1(x)\n",
        "        x2 = self.body2(x1)\n",
        "        x3 = self.body3(x2)\n",
        "\n",
        "        # x = self.output_layer(x)\n",
        "        # return l2_norm(x)\n",
        "\n",
        "        return x1, x2, x3\n",
        "\n",
        "def load_pretrained_weights(model, checkpoint):\n",
        "    import collections\n",
        "    if 'state_dict' in checkpoint:\n",
        "        state_dict = checkpoint['state_dict']\n",
        "    else:\n",
        "        state_dict = checkpoint\n",
        "    model_dict = model.state_dict()\n",
        "    new_state_dict = collections.OrderedDict()\n",
        "    matched_layers, discarded_layers = [], []\n",
        "    for i, (k, v) in enumerate(state_dict.items()):\n",
        "        # print(i)\n",
        "\n",
        "        # If the pretrained state_dict was saved as nn.DataParallel,\n",
        "        # keys would contain \"module.\", which should be ignored.\n",
        "        if k.startswith('module.'):\n",
        "            k = k[7:]\n",
        "        if k in model_dict and model_dict[k].size() == v.size():\n",
        "\n",
        "            new_state_dict[k] = v\n",
        "            matched_layers.append(k)\n",
        "        else:\n",
        "            # print(k)\n",
        "            discarded_layers.append(k)\n",
        "    # new_state_dict.requires_grad = False\n",
        "    model_dict.update(new_state_dict)\n",
        "    model.load_state_dict(model_dict)\n",
        "    print('load_weight', len(matched_layers))\n",
        "    return model\n",
        "\n",
        "# model = Backbone(50, 0.0, 'ir')\n",
        "# ir_checkpoint = torch.load(r'C:\\Users\\86187\\Desktop\\project\\mixfacial\\models\\pretrain\\new_ir50.pth')\n",
        "# print('hello')\n",
        "# i1, i2, i3 = 0, 0, 0\n",
        "# ir_checkpoint = torch.load(r'C:\\Users\\86187\\Desktop\\project\\mixfacial\\models\\pretrain\\ir50.pth', map_location=lambda storage, loc: storage)\n",
        "# for (k1, v1), (k2, v2) in zip(model.state_dict().items(), ir_checkpoint.items()):\n",
        "#     print(f'k1:{k1}, k2:{k2}')\n",
        "#     model.state_dict()[k1] = v2\n",
        "\n",
        "# torch.save(model.state_dict(), r'C:\\Users\\86187\\Desktop\\project\\mixfacial\\models\\pretrain\\new_ir50.pth')\n",
        "#     print(k)\n",
        "#     if k.startswith('body1'):\n",
        "#         i1+=1\n",
        "#     if k.startswith('body2'):\n",
        "#         i2+=1\n",
        "#     if k.startswith('body3'):\n",
        "#         i3+=1\n",
        "# print(f'i1:{i1}, i2:{i2}, i3:{i3}')\n",
        "\n",
        "# print('-'*100)\n",
        "# ir_checkpoint = torch.load(r'C:\\Users\\86187\\Desktop\\project\\mixfacial\\models\\pretrain\\ir50.pth', map_location=lambda storage, loc: storage)\n",
        "# le = 0\n",
        "# for k, v in ir_checkpoint.items():\n",
        "#     # print(k)\n",
        "#     if k.startswith('body'):\n",
        "#         if le < i1:\n",
        "#             le += 1\n",
        "#             key = k.split('.')[0] + str(1) + k.split('.')[1:]\n",
        "#             print(key)\n",
        "# # ir_checkpoint = ir_checkpoint[\"model\"]\n",
        "# model = load_pretrained_weights(model, ir_checkpoint)\n",
        "# img = torch.rand(size=(2,3,224,224))\n",
        "# out1, out2, out3 = model(img)\n",
        "# print(out1.shape, out2.shape, out3.shape)"
      ],
      "metadata": {
        "id": "3F2v1o5gtQ-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### models/matrix.py"
      ],
      "metadata": {
        "id": "56AEsSIAtlj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "\n",
        "# -*- coding:utf-8 -*-\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, fontsize=16)\n",
        "    plt.yticks(tick_marks, classes, fontsize=16)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True Label',fontsize=12)\n",
        "    plt.xlabel('Predicted Label',fontsize=12)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "cnf_matrix = np.array([[ 299 ,   6 ,   5 ,   3 ,   1 ,   4,   11],\n",
        " [   9,   51   , 0,    2   , 8,    2   , 2],\n",
        " [   2 ,   1  ,120 ,   6   ,13 ,   9  ,  9],\n",
        " [   5  ,  1   , 7 ,1148   , 2  ,  4 ,  18],\n",
        " [   0   , 0  ,  9  ,  4  ,442   , 1  , 22],\n",
        " [   2    ,0 ,   7   , 3 ,   0  ,145 ,   5],\n",
        " [  10    ,0,    6   ,11,   29   , 0,  624]])\n",
        "\n",
        "class_names = [\"SU\", 'FE', 'AN', 'HA', 'SA', 'DI', 'NE']\n",
        "\n",
        "\n",
        "plt.figure(dpi=200)\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
        "                      title=None)"
      ],
      "metadata": {
        "id": "EZfK-PAytnu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### models/mobilefacenet.py"
      ],
      "metadata": {
        "id": "cEQTyNOntqUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import Linear, Conv2d, BatchNorm1d, BatchNorm2d, PReLU, ReLU, Sigmoid, Dropout2d, Dropout, AvgPool2d, \\\n",
        "    MaxPool2d, AdaptiveAvgPool2d, Sequential, Module, Parameter\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import namedtuple\n",
        "import math\n",
        "import pdb\n",
        "\n",
        "\n",
        "##################################  Original Arcface Model #############################################################\n",
        "######## ccc#######################\n",
        "class Flatten(Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1)\n",
        "\n",
        "\n",
        "##################################  MobileFaceNet #############################################################\n",
        "\n",
        "class Conv_block(Module):\n",
        "    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n",
        "        super(Conv_block, self).__init__()\n",
        "        self.conv = Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding,\n",
        "                           bias=False)\n",
        "        self.bn = BatchNorm2d(out_c)\n",
        "        self.prelu = PReLU(out_c)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.prelu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Linear_block(Module):\n",
        "    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n",
        "        super(Linear_block, self).__init__()\n",
        "        self.conv = Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding,\n",
        "                           bias=False)\n",
        "        self.bn = BatchNorm2d(out_c)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Depth_Wise(Module):\n",
        "    def __init__(self, in_c, out_c, residual=False, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=1):\n",
        "        super(Depth_Wise, self).__init__()\n",
        "        self.conv = Conv_block(in_c, out_c=groups, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n",
        "        self.conv_dw = Conv_block(groups, groups, groups=groups, kernel=kernel, padding=padding, stride=stride)\n",
        "        self.project = Linear_block(groups, out_c, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n",
        "        self.residual = residual\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.residual:\n",
        "            short_cut = x\n",
        "        x = self.conv(x)\n",
        "        x = self.conv_dw(x)\n",
        "        x = self.project(x)\n",
        "        if self.residual:\n",
        "            output = short_cut + x\n",
        "        else:\n",
        "            output = x\n",
        "        return output\n",
        "\n",
        "\n",
        "class Residual(Module):\n",
        "    def __init__(self, c, num_block, groups, kernel=(3, 3), stride=(1, 1), padding=(1, 1)):\n",
        "        super(Residual, self).__init__()\n",
        "        modules = []\n",
        "        for _ in range(num_block):\n",
        "            modules.append(\n",
        "                Depth_Wise(c, c, residual=True, kernel=kernel, padding=padding, stride=stride, groups=groups))\n",
        "        self.model = Sequential(*modules)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class GNAP(Module):\n",
        "    def __init__(self, embedding_size):\n",
        "        super(GNAP, self).__init__()\n",
        "        assert embedding_size == 512\n",
        "        self.bn1 = BatchNorm2d(512, affine=False)\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        self.bn2 = BatchNorm1d(512, affine=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.bn1(x)\n",
        "        x_norm = torch.norm(x, 2, 1, True)\n",
        "        x_norm_mean = torch.mean(x_norm)\n",
        "        weight = x_norm_mean / x_norm\n",
        "        x = x * weight\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        feature = self.bn2(x)\n",
        "        return feature\n",
        "\n",
        "\n",
        "class GDC(Module):\n",
        "    def __init__(self, embedding_size):\n",
        "        super(GDC, self).__init__()\n",
        "        self.conv_6_dw = Linear_block(512, 512, groups=512, kernel=(7, 7), stride=(1, 1), padding=(0, 0))\n",
        "        self.conv_6_flatten = Flatten()\n",
        "        self.linear = Linear(512, embedding_size, bias=False)\n",
        "        # self.bn = BatchNorm1d(embedding_size, affine=False)\n",
        "        self.bn = BatchNorm1d(embedding_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_6_dw(x)    #### [B, 512, 1, 1]\n",
        "        x = self.conv_6_flatten(x)   #### [B, 512]\n",
        "        x = self.linear(x)      #### [B, 136]\n",
        "        x = self.bn(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MobileFaceNet(Module):\n",
        "    def __init__(self, input_size, embedding_size=512, output_name=\"GDC\"):\n",
        "        super(MobileFaceNet, self).__init__()\n",
        "        assert output_name in [\"GNAP\", 'GDC']\n",
        "        assert input_size[0] in [112]\n",
        "        self.conv1 = Conv_block(3, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "        self.conv2_dw = Conv_block(64, 64, kernel=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
        "        self.conv_23 = Depth_Wise(64, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=128)\n",
        "        self.conv_3 = Residual(64, num_block=4, groups=128, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv_34 = Depth_Wise(64, 128, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
        "        self.conv_4 = Residual(128, num_block=6, groups=256, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv_45 = Depth_Wise(128, 128, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)\n",
        "        self.conv_5 = Residual(128, num_block=2, groups=256, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv_6_sep = Conv_block(128, 512, kernel=(1, 1), stride=(1, 1), padding=(0, 0))\n",
        "        if output_name == \"GNAP\":\n",
        "            self.output_layer = GNAP(512)\n",
        "        else:\n",
        "            self.output_layer = GDC(embedding_size)\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        # print(out.shape)\n",
        "        out = self.conv2_dw(out)\n",
        "        # print(out.shape)\n",
        "        out = self.conv_23(out)\n",
        "        # print(out.shape)\n",
        "        out3 = self.conv_3(out)\n",
        "        # print(out.shape)\n",
        "        out = self.conv_34(out3)\n",
        "        # print(out.shape)\n",
        "        out4 = self.conv_4(out)  # [128, 14, 14]\n",
        "        # print(out.shape)\n",
        "        out = self.conv_45(out4)  # [128, 7, 7]\n",
        "        # print(out.shape)\n",
        "        out = self.conv_5(out)  # [128, 7, 7]\n",
        "        # print(out.shape)\n",
        "        conv_features = self.conv_6_sep(out)    ##### [B, 512, 7, 7]\n",
        "        out = self.output_layer(conv_features)  ##### [B, 136]\n",
        "        return out3, out4, conv_features\n",
        "\n",
        "\n",
        "# model = MobileFaceNet([112, 112],136)\n",
        "# input = torch.ones(8,3,112,112).cuda()\n",
        "# model = model.cuda()\n",
        "# x = model(input)\n",
        "# import numpy as np\n",
        "# parameters = model.parameters()\n",
        "# parameters = sum([np.prod(p.size()) for p in parameters]) / 1_000_000\n",
        "# print('Total Parameters: %.3fM' % parameters)\n",
        "#\n",
        "#\n",
        "# from ptflops import get_model_complexity_info\n",
        "# macs, params = get_model_complexity_info(model, (3, 112, 112), as_strings=True,\n",
        "#                                        print_per_layer_stat=True, verbose=True)\n",
        "# print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
        "# print('{:<30}  {:<8}'.format('Number of parameters: ', params))\n",
        "#\n",
        "# print(x.shape)"
      ],
      "metadata": {
        "id": "2tg52Sabtsrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### models/PosterV2_7cls.py"
      ],
      "metadata": {
        "id": "hlHwQwq9ttMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from .mobilefacenet import MobileFaceNet\n",
        "from .ir50 import Backbone\n",
        "from .vit_model import VisionTransformer, PatchEmbed\n",
        "from timm.models.layers import trunc_normal_, DropPath\n",
        "from thop import profile\n",
        "\n",
        "\n",
        "def load_pretrained_weights(model, checkpoint):\n",
        "    import collections\n",
        "    if 'state_dict' in checkpoint:\n",
        "        state_dict = checkpoint['state_dict']\n",
        "    else:\n",
        "        state_dict = checkpoint\n",
        "    model_dict = model.state_dict()\n",
        "    new_state_dict = collections.OrderedDict()\n",
        "    matched_layers, discarded_layers = [], []\n",
        "    for k, v in state_dict.items():\n",
        "        # If the pretrained state_dict was saved as nn.DataParallel,\n",
        "        # keys would contain \"module.\", which should be ignored.\n",
        "        if k.startswith('module.'):\n",
        "            k = k[7:]\n",
        "        if k in model_dict and model_dict[k].size() == v.size():\n",
        "            new_state_dict[k] = v\n",
        "            matched_layers.append(k)\n",
        "        else:\n",
        "            discarded_layers.append(k)\n",
        "    # new_state_dict.requires_grad = False\n",
        "    model_dict.update(new_state_dict)\n",
        "\n",
        "    model.load_state_dict(model_dict)\n",
        "    print('load_weight', len(matched_layers))\n",
        "    return model\n",
        "\n",
        "def window_partition(x, window_size, h_w, w_w):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: (B, H, W, C)\n",
        "        window_size: window size\n",
        "\n",
        "    Returns:\n",
        "        local window features (num_windows*B, window_size, window_size, C)\n",
        "    \"\"\"\n",
        "    B, H, W, C = x.shape\n",
        "    x = x.view(B, h_w, window_size, w_w, window_size, C)\n",
        "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
        "    return windows\n",
        "\n",
        "class window(nn.Module):\n",
        "    def __init__(self, window_size, dim):\n",
        "        super(window, self).__init__()\n",
        "        self.window_size = window_size\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        B, H, W, C = x.shape\n",
        "        x = self.norm(x)\n",
        "        shortcut = x\n",
        "        h_w = int(torch.div(H, self.window_size).item())\n",
        "        w_w = int(torch.div(W, self.window_size).item())\n",
        "        x_windows = window_partition(x, self.window_size, h_w, w_w)\n",
        "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)\n",
        "        return x_windows, shortcut\n",
        "\n",
        "class WindowAttentionGlobal(nn.Module):\n",
        "    \"\"\"\n",
        "    Global window attention based on: \"Hatamizadeh et al.,\n",
        "    Global Context Vision Transformers <https://arxiv.org/abs/2206.09959>\"\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 dim,\n",
        "                 num_heads,\n",
        "                 window_size,\n",
        "                 qkv_bias=True,\n",
        "                 qk_scale=None,\n",
        "                 attn_drop=0.,\n",
        "                 proj_drop=0.,\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dim: feature size dimension.\n",
        "            num_heads: number of attention head.\n",
        "            window_size: window size.\n",
        "            qkv_bias: bool argument for query, key, value learnable bias.\n",
        "            qk_scale: bool argument to scaling query, key.\n",
        "            attn_drop: attention dropout rate.\n",
        "            proj_drop: output dropout rate.\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        window_size = (window_size, window_size)\n",
        "        self.window_size = window_size\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = torch.div(dim, num_heads)\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "        self.relative_position_bias_table = nn.Parameter(\n",
        "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))\n",
        "        coords_h = torch.arange(self.window_size[0])\n",
        "        coords_w = torch.arange(self.window_size[1])\n",
        "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))\n",
        "        coords_flatten = torch.flatten(coords, 1)\n",
        "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
        "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
        "        relative_coords[:, :, 0] += self.window_size[0] - 1\n",
        "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
        "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
        "        relative_position_index = relative_coords.sum(-1)\n",
        "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
        "        self.qkv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x, q_global):\n",
        "        # print(f'q_global.shape:{q_global.shape}')\n",
        "        # print(f'x.shape:{x.shape}')\n",
        "        B_, N, C = x.shape\n",
        "        B = q_global.shape[0]\n",
        "        head_dim = int(torch.div(C, self.num_heads).item())\n",
        "        B_dim = int(torch.div(B_, B).item())\n",
        "        kv = self.qkv(x).reshape(B_, N, 2, self.num_heads, head_dim).permute(2, 0, 3, 1, 4)\n",
        "        k, v = kv[0], kv[1]\n",
        "        q_global = q_global.repeat(1, B_dim, 1, 1, 1)\n",
        "        q = q_global.reshape(B_, self.num_heads, N, head_dim)\n",
        "        q = q * self.scale\n",
        "        attn = (q @ k.transpose(-2, -1))\n",
        "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
        "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)\n",
        "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()\n",
        "        attn = attn + relative_position_bias.unsqueeze(0)\n",
        "        attn = self.softmax(attn)\n",
        "        attn = self.attn_drop(attn)\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "def _to_channel_last(x):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: (B, C, H, W)\n",
        "\n",
        "    Returns:\n",
        "        x: (B, H, W, C)\n",
        "    \"\"\"\n",
        "    return x.permute(0, 2, 3, 1)\n",
        "\n",
        "def _to_channel_first(x):\n",
        "    return x.permute(0, 3, 1, 2)\n",
        "\n",
        "def _to_query(x, N, num_heads, dim_head):\n",
        "    B = x.shape[0]\n",
        "    x = x.reshape(B, 1, N, num_heads, dim_head).permute(0, 1, 3, 2, 4)\n",
        "    return x\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Layer Perceptron (MLP) block\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_features,\n",
        "                 hidden_features=None,\n",
        "                 out_features=None,\n",
        "                 act_layer=nn.GELU,\n",
        "                 drop=0.):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            in_features: input features dimension.\n",
        "            hidden_features: hidden features dimension.\n",
        "            out_features: output features dimension.\n",
        "            act_layer: activation function.\n",
        "            drop: dropout rate.\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "def window_reverse(windows, window_size, H, W, h_w, w_w):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        windows: local window features (num_windows*B, window_size, window_size, C)\n",
        "        window_size: Window size\n",
        "        H: Height of image\n",
        "        W: Width of image\n",
        "\n",
        "    Returns:\n",
        "        x: (B, H, W, C)\n",
        "    \"\"\"\n",
        "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
        "    x = windows.view(B, h_w, w_w, window_size, window_size, -1)\n",
        "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
        "    return x\n",
        "\n",
        "class feedforward(nn.Module):\n",
        "    def __init__(self, dim, window_size, mlp_ratio=4., act_layer=nn.GELU, drop=0., drop_path=0., layer_scale=None):\n",
        "        super(feedforward, self).__init__()\n",
        "        if layer_scale is not None and type(layer_scale) in [int, float]:\n",
        "            self.layer_scale = True\n",
        "            self.gamma1 = nn.Parameter(layer_scale * torch.ones(dim), requires_grad=True)\n",
        "            self.gamma2 = nn.Parameter(layer_scale * torch.ones(dim), requires_grad=True)\n",
        "        else:\n",
        "            self.gamma1 = 1.0\n",
        "            self.gamma2 = 1.0\n",
        "        self.window_size = window_size\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop)\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "    def forward(self, attn_windows, shortcut):\n",
        "        B, H, W, C = shortcut.shape\n",
        "        h_w = int(torch.div(H, self.window_size).item())\n",
        "        w_w = int(torch.div(W, self.window_size).item())\n",
        "        x = window_reverse(attn_windows, self.window_size, H, W, h_w, w_w)\n",
        "        x = shortcut + self.drop_path(self.gamma1 * x)\n",
        "        x = x + self.drop_path(self.gamma2 * self.mlp(self.norm(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class pyramid_trans_expr2(nn.Module):\n",
        "    def __init__(self, img_size=224, num_classes=7, window_size=[28,14,7], num_heads=[2, 4, 8], dims=[64, 128, 256], embed_dim=768):\n",
        "        super().__init__()\n",
        "\n",
        "        self.img_size = img_size\n",
        "        self.num_heads = num_heads\n",
        "        self.dim_head = []\n",
        "        for num_head, dim in zip(num_heads, dims):\n",
        "            self.dim_head.append(int(torch.div(dim, num_head).item()))\n",
        "        self.num_classes = num_classes\n",
        "        self.window_size = window_size\n",
        "        self.N = [win * win for win in window_size]\n",
        "        self.face_landback = MobileFaceNet([112, 112], 136)\n",
        "        face_landback_checkpoint = torch.load(r'./models/pretrain/mobilefacenet_model_best.pth.tar',\n",
        "                                              map_location=lambda storage, loc: storage)\n",
        "        self.face_landback.load_state_dict(face_landback_checkpoint['state_dict'])\n",
        "\n",
        "        for param in self.face_landback.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.VIT = VisionTransformer(depth=2, embed_dim=embed_dim)\n",
        "\n",
        "        self.ir_back = Backbone(50, 0.0, 'ir')\n",
        "        ir_checkpoint = torch.load(r'./models/pretrain/ir50.pth', map_location=lambda storage, loc: storage)\n",
        "\n",
        "        self.ir_back = load_pretrained_weights(self.ir_back, ir_checkpoint)\n",
        "\n",
        "        self.attn1 = WindowAttentionGlobal(dim=dims[0], num_heads=num_heads[0], window_size=window_size[0])\n",
        "        self.attn2 = WindowAttentionGlobal(dim=dims[1], num_heads=num_heads[1], window_size=window_size[1])\n",
        "        self.attn3 = WindowAttentionGlobal(dim=dims[2], num_heads=num_heads[2], window_size=window_size[2])\n",
        "        self.window1 = window(window_size=window_size[0], dim=dims[0])\n",
        "        self.window2 = window(window_size=window_size[1], dim=dims[1])\n",
        "        self.window3 = window(window_size=window_size[2], dim=dims[2])\n",
        "        self.conv1 = nn.Conv2d(in_channels=dims[0], out_channels=dims[0], kernel_size=3, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=dims[1], out_channels=dims[1], kernel_size=3, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=dims[2], out_channels=dims[2], kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        dpr = [x.item() for x in torch.linspace(0, 0.5, 5)]\n",
        "        self.ffn1 = feedforward(dim=dims[0], window_size=window_size[0], layer_scale=1e-5, drop_path=dpr[0])\n",
        "        self.ffn2 = feedforward(dim=dims[1], window_size=window_size[1], layer_scale=1e-5, drop_path=dpr[1])\n",
        "        self.ffn3 = feedforward(dim=dims[2], window_size=window_size[2], layer_scale=1e-5, drop_path=dpr[2])\n",
        "\n",
        "        self.last_face_conv = nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, padding=1)\n",
        "\n",
        "        self.embed_q = nn.Sequential(nn.Conv2d(dims[0], 768, kernel_size=3, stride=2, padding=1),\n",
        "                                     nn.Conv2d(768, 768, kernel_size=3, stride=2, padding=1))\n",
        "        self.embed_k = nn.Sequential(nn.Conv2d(dims[1], 768, kernel_size=3, stride=2, padding=1))\n",
        "        self.embed_v = PatchEmbed(img_size=14, patch_size=14, in_c=256, embed_dim=768)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_face = F.interpolate(x, size=112)\n",
        "        x_face1 , x_face2, x_face3 = self.face_landback(x_face)\n",
        "        x_face3 = self.last_face_conv(x_face3)\n",
        "        x_face1, x_face2, x_face3 = _to_channel_last(x_face1), _to_channel_last(x_face2), _to_channel_last(x_face3)\n",
        "\n",
        "        q1, q2, q3 = _to_query(x_face1, self.N[0], self.num_heads[0], self.dim_head[0]), \\\n",
        "                     _to_query(x_face2, self.N[1], self.num_heads[1], self.dim_head[1]), \\\n",
        "                     _to_query(x_face3, self.N[2], self.num_heads[2], self.dim_head[2])\n",
        "\n",
        "        x_ir1, x_ir2, x_ir3 = self.ir_back(x)\n",
        "\n",
        "        x_ir1, x_ir2, x_ir3 = self.conv1(x_ir1), self.conv2(x_ir2), self.conv3(x_ir3)\n",
        "        x_window1, shortcut1 = self.window1(x_ir1)\n",
        "        x_window2, shortcut2 = self.window2(x_ir2)\n",
        "        x_window3, shortcut3 = self.window3(x_ir3)\n",
        "\n",
        "        o1, o2, o3 = self.attn1(x_window1, q1), self.attn2(x_window2, q2), self.attn3(x_window3, q3)\n",
        "\n",
        "        o1, o2, o3 = self.ffn1(o1, shortcut1), self.ffn2(o2, shortcut2), self.ffn3(o3, shortcut3)\n",
        "\n",
        "        o1, o2, o3 = _to_channel_first(o1), _to_channel_first(o2), _to_channel_first(o3)\n",
        "\n",
        "        o1, o2, o3 = self.embed_q(o1).flatten(2).transpose(1, 2), self.embed_k(o2).flatten(2).transpose(1, 2), self.embed_v(o3)\n",
        "\n",
        "        o = torch.cat([o1, o2, o3], dim=1)\n",
        "\n",
        "        out = self.VIT(o)\n",
        "        return out\n",
        "\n",
        "def compute_param_flop():\n",
        "    model = pyramid_trans_expr2()\n",
        "    img = torch.rand(size=(1,3,224,224))\n",
        "    flops, params = profile(model, inputs=(img,))\n",
        "    print(f'flops:{flops/1000**3}G,params:{params/1000**2}M')\n",
        "\n"
      ],
      "metadata": {
        "id": "2mOoze7WtxOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### models/vit_model.py"
      ],
      "metadata": {
        "id": "gWautH9wtxqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "original code from rwightman:\n",
        "https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
        "\"\"\"\n",
        "from functools import partial\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.hub\n",
        "from functools import partial\n",
        "# import mat\n",
        "# from vision_transformer.ir50 import Backbone\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.hub\n",
        "from functools import partial\n",
        "import math\n",
        "\n",
        "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
        "from timm.models.registry import register_model\n",
        "from timm.models.vision_transformer import _cfg, Mlp, Block\n",
        "# from .ir50 import Backbone\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
        "    \"\"\"\n",
        "    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
        "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
        "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
        "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
        "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
        "    'survival rate' as the argument.\n",
        "    \"\"\"\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    __constants__ = ['downsample']\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        norm_layer = nn.BatchNorm2d\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"\n",
        "    Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\"\n",
        "    2D Image to Patch Embedding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, img_size=14, patch_size=16, in_c=256, embed_dim=768, norm_layer=None):\n",
        "        super().__init__()\n",
        "        img_size = (img_size, img_size)\n",
        "        patch_size = (patch_size, patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n",
        "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
        "\n",
        "        self.proj = nn.Conv2d(256, 768, kernel_size=1)\n",
        "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        # assert H == self.img_size[0] and W == self.img_size[1], \\\n",
        "        #     f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
        "        # print(x.shape)\n",
        "\n",
        "        # flatten: [B, C, H, W] -> [B, C, HW]\n",
        "        # transpose: [B, C, HW] -> [B, HW, C]\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self,\n",
        "                 dim, in_chans,  # 输入token的dim\n",
        "                 num_heads=8,\n",
        "                 qkv_bias=False,\n",
        "                 qk_scale=None,\n",
        "                 attn_drop_ratio=0.,\n",
        "                 proj_drop_ratio=0.):\n",
        "        super(Attention, self).__init__()\n",
        "        self.num_heads = 8\n",
        "        self.img_chanel = in_chans + 1\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop_ratio)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop_ratio)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_img = x[:, :self.img_chanel, :]\n",
        "        # [batch_size, num_patches + 1, total_embed_dim]\n",
        "        B, N, C = x_img.shape\n",
        "        # print(C)\n",
        "        qkv = self.qkv(x_img).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "        # k, v = kv.unbind(0)  # make torchscript happy (cannot use tensor as tuple)\n",
        "        # q = x_img.reshape(B, -1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x_img = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x_img = self.proj(x_img)\n",
        "        x_img = self.proj_drop(x_img)\n",
        "        #\n",
        "        #\n",
        "        # # qkv(): -> [batch_size, num_patches + 1, 3 * total_embed_dim]\n",
        "        # # reshape: -> [batch_size, num_patches + 1, 3, num_heads, embed_dim_per_head]\n",
        "        # # permute: -> [3, batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
        "        # qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        # # [batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
        "        # q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
        "        #\n",
        "        # # transpose: -> [batch_size, num_heads, embed_dim_per_head, num_patches + 1]\n",
        "        # # @: multiply -> [batch_size, num_heads, num_patches + 1, num_patches + 1]\n",
        "        # attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        # attn = attn.softmax(dim=-1)\n",
        "        # attn = self.attn_drop(attn)\n",
        "        #\n",
        "        # # @: multiply -> [batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
        "        # # transpose: -> [batch_size, num_patches + 1, num_heads, embed_dim_per_head]\n",
        "        # # reshape: -> [batch_size, num_patches + 1, total_embed_dim]\n",
        "        # x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        # x = self.proj(x)\n",
        "        # x = self.proj_drop(x)\n",
        "        return x_img\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    __constants__ = ['downsample']\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(AttentionBlock, self).__init__()\n",
        "        norm_layer = nn.BatchNorm2d\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = norm_layer(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = norm_layer(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        # self.cbam = CBAM(planes, 16)\n",
        "        self.inplanes = inplanes\n",
        "        self.eca_block = eca_block()\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        inplanes = self.inplanes\n",
        "        out = self.eca_block(out)\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "    \"\"\"\n",
        "    MLP as used in Vision Transformer, MLP-Mixer and related networks\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self,\n",
        "                 dim, in_chans,\n",
        "                 num_heads,\n",
        "                 mlp_ratio=4.,\n",
        "                 qkv_bias=False,\n",
        "                 qk_scale=None,\n",
        "                 drop_ratio=0.,\n",
        "                 attn_drop_ratio=0.,\n",
        "                 drop_path_ratio=0.,\n",
        "                 act_layer=nn.GELU,\n",
        "                 norm_layer=nn.LayerNorm):\n",
        "        super(Block, self).__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.img_chanel = in_chans + 1\n",
        "\n",
        "        self.conv = nn.Conv1d(self.img_chanel, self.img_chanel, 1)\n",
        "        self.attn = Attention(dim, in_chans=in_chans, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                              attn_drop_ratio=attn_drop_ratio, proj_drop_ratio=drop_ratio)\n",
        "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
        "        self.drop_path = DropPath(drop_path_ratio) if drop_path_ratio > 0. else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop_ratio)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = x + self.drop_path(self.attn(self.norm1(x)))\n",
        "        # x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "\n",
        "        x_img = x\n",
        "        # [:, :self.img_chanel, :]\n",
        "        # x_lm = x[:, self.img_chanel:, :]\n",
        "        x_img = x_img + self.drop_path(self.attn(self.norm1(x)))\n",
        "        x = x_img + self.drop_path(self.mlp(self.norm2(x_img)))\n",
        "        #\n",
        "        # x_lm = x_lm + self.drop_path(self.attn_lm(self.norm3(x)))\n",
        "        # x_lm = x_lm + self.drop_path(self.mlp2(self.norm4(x_lm)))\n",
        "        # x = torch.cat((x_img, x_lm), dim=1)\n",
        "        # x = self.conv(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class ClassificationHead(nn.Module):\n",
        "    def __init__(self, input_dim: int, target_dim: int):\n",
        "        super().__init__()\n",
        "        self.linear = torch.nn.Linear(input_dim, target_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        y_hat = self.linear(x)\n",
        "        return y_hat\n",
        "\n",
        "\n",
        "def load_pretrained_weights(model, checkpoint):\n",
        "    import collections\n",
        "    if 'state_dict' in checkpoint:\n",
        "        state_dict = checkpoint['state_dict']\n",
        "    else:\n",
        "        state_dict = checkpoint\n",
        "    model_dict = model.state_dict()\n",
        "    new_state_dict = collections.OrderedDict()\n",
        "    matched_layers, discarded_layers = [], []\n",
        "    for k, v in state_dict.items():\n",
        "        # If the pretrained state_dict was saved as nn.DataParallel,\n",
        "        # keys would contain \"module.\", which should be ignored.\n",
        "        if k.startswith('module.'):\n",
        "            k = k[7:]\n",
        "        if k in model_dict and model_dict[k].size() == v.size():\n",
        "            new_state_dict[k] = v\n",
        "            matched_layers.append(k)\n",
        "        else:\n",
        "            discarded_layers.append(k)\n",
        "    # new_state_dict.requires_grad = False\n",
        "    model_dict.update(new_state_dict)\n",
        "\n",
        "    model.load_state_dict(model_dict)\n",
        "    print('load_weight', len(matched_layers))\n",
        "    return model\n",
        "\n",
        "class eca_block(nn.Module):\n",
        "    def __init__(self, channel=128, b=1, gamma=2):\n",
        "        super(eca_block, self).__init__()\n",
        "        kernel_size = int(abs((math.log(channel, 2) + b) / gamma))\n",
        "        kernel_size = kernel_size if kernel_size % 2 else kernel_size + 1\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.conv = nn.Conv1d(1, 1, kernel_size=kernel_size, padding=(kernel_size - 1) // 2, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.avg_pool(x)\n",
        "        y = self.conv(y.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)\n",
        "        y = self.sigmoid(y)\n",
        "        return x * y.expand_as(x)\n",
        "#\n",
        "#\n",
        "# class IR20(nn.Module):\n",
        "#     def __init__(self, img_size_=112, num_classes=7, layers=[2, 2, 2, 2]):\n",
        "#         super().__init__()\n",
        "#         norm_layer = nn.BatchNorm2d\n",
        "#         self.img_size = img_size_\n",
        "#         self._norm_layer = norm_layer\n",
        "#         self.num_classes = num_classes\n",
        "#         self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "#         self.bn1 = norm_layer(64)\n",
        "#         self.relu = nn.ReLU(inplace=True)\n",
        "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "#         # self.face_landback = MobileFaceNet([112, 112],136)\n",
        "#         # face_landback_checkpoint = torch.load('./models/pretrain/mobilefacenet_model_best.pth.tar', map_location=lambda storage, loc: storage)\n",
        "#         # self.face_landback.load_state_dict(face_landback_checkpoint['state_dict'])\n",
        "#         self.layer1 = self._make_layer(BasicBlock, 64, 64, layers[0])\n",
        "#         self.layer2 = self._make_layer(BasicBlock, 64, 128, layers[1], stride=2)\n",
        "#         self.layer3 = self._make_layer(AttentionBlock, 128, 256, layers[2], stride=2)\n",
        "#         self.layer4 = self._make_layer(AttentionBlock, 256, 256, layers[3], stride=1)\n",
        "#         self.ir_back = Backbone(50, 51, 52, 0.0, 'ir')\n",
        "#         self.ir_layer = nn.Linear(1024, 512)\n",
        "#         # ir_checkpoint = torch.load(r'F:\\0815crossvit\\vision_transformer\\models\\pretrain\\Pretrained_on_MSCeleb.pth.tar',\n",
        "#         #                          map_location=lambda storage, loc: storage)\n",
        "#         # ir_checkpoint = ir_checkpoint['state_dict']\n",
        "#         # self.face_landback.load_state_dict(face_landback_checkpoint['state_dict'])\n",
        "#         # checkpoint = torch.load('./checkpoint/Pretrained_on_MSCeleb.pth.tar')\n",
        "#         # pre_trained_dict = checkpoint['state_dict']\n",
        "#         # IR20.load_state_dict(ir_checkpoint, strict=False)\n",
        "#         # self.IR = load_pretrained_weights(IR, ir_checkpoint)\n",
        "#\n",
        "#     def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n",
        "#         norm_layer = self._norm_layer\n",
        "#         downsample = None\n",
        "#         if stride != 1 or inplanes != planes:\n",
        "#             downsample = nn.Sequential(conv1x1(inplanes, planes, stride), norm_layer(planes))\n",
        "#         layers = []\n",
        "#         layers.append(block(inplanes, planes, stride, downsample))\n",
        "#         inplanes = planes\n",
        "#         for _ in range(1, blocks):\n",
        "#             layers.append(block(inplanes, planes))\n",
        "#         return nn.Sequential(*layers)\n",
        "#\n",
        "#     def forward(self, x):\n",
        "#         x_ir = self.ir_back(x)\n",
        "#         # x_ir = self.ir_layer(x_ir)\n",
        "#         # print(x_ir.shape)\n",
        "#         # x = F.interpolate(x, size=112)\n",
        "#         # x = self.conv1(x)\n",
        "#         # x = self.bn1(x)\n",
        "#         # x = self.relu(x)\n",
        "#         # x = self.maxpool(x)\n",
        "#         #\n",
        "#         # x = self.layer1(x)\n",
        "#         # x = self.layer2(x)\n",
        "#         # x = self.layer3(x)\n",
        "#         # x = self.layer4(x)\n",
        "#         # print(x.shape)\n",
        "#         # print(x)\n",
        "#         out = x_ir\n",
        "#\n",
        "#         return out\n",
        "#\n",
        "#\n",
        "# class IR(nn.Module):\n",
        "#     def __init__(self, img_size_=112, num_classes=7):\n",
        "#         super().__init__()\n",
        "#         depth = 8\n",
        "#         # if type == \"small\":\n",
        "#         #     depth = 4\n",
        "#         # if type == \"base\":\n",
        "#         #     depth = 6\n",
        "#         # if type == \"large\":\n",
        "#         #     depth = 8\n",
        "#\n",
        "#         self.img_size = img_size_\n",
        "#         self.num_classes = num_classes\n",
        "#         self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "#         # self.bn1 = norm_layer(64)\n",
        "#         self.relu = nn.ReLU(inplace=True)\n",
        "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "#         # self.face_landback = MobileFaceNet([112, 112],136)\n",
        "#         # face_landback_checkpoint = torch.load('./models/pretrain/mobilefacenet_model_best.pth.tar', map_location=lambda storage, loc: storage)\n",
        "#         # self.face_landback.load_state_dict(face_landback_checkpoint['state_dict'])\n",
        "#\n",
        "#         # for param in self.face_landback.parameters():\n",
        "#         #     param.requires_grad = False\n",
        "#\n",
        "#         ###########################################################################333\n",
        "#\n",
        "#         self.ir_back = IR20()\n",
        "#\n",
        "#         # ir_checkpoint = torch.load(r'F:\\0815crossvit\\vision_transformer\\models\\pretrain\\ir50.pth',\n",
        "#         #                            map_location=lambda storage, loc: storage)\n",
        "#         # # ir_checkpoint = ir_checkpoint[\"model\"]\n",
        "#         # self.ir_back = load_pretrained_weights(self.ir_back, ir_checkpoint)\n",
        "#         # load_state_dict(checkpoint_model, strict=False)\n",
        "#         # self.ir_layer = nn.Linear(1024,512)\n",
        "#\n",
        "#         #############################################################3\n",
        "#         #\n",
        "#         # self.pyramid_fuse = HyVisionTransformer(in_chans=49, q_chanel = 49, embed_dim=512,\n",
        "#         #                                      depth=depth, num_heads=8, mlp_ratio=2.,\n",
        "#         #                                      drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1)\n",
        "#\n",
        "#         # self.se_block = SE_block(input_dim=512)\n",
        "#         self.head = ClassificationHead(input_dim=768, target_dim=self.num_classes)\n",
        "#\n",
        "#     def forward(self, x):\n",
        "#         B_ = x.shape[0]\n",
        "#         # x_face = F.interpolate(x, size=112)\n",
        "#         # _, x_face = self.face_landback(x_face)\n",
        "#         # x_face = x_face.view(B_, -1, 49).transpose(1,2)\n",
        "#         ###############  landmark x_face ([B, 49, 512])\n",
        "#         x_ir = self.ir_back(x)\n",
        "#         # print(x_ir.shape)\n",
        "#         # x_ir = self.ir_layer(x_ir)\n",
        "#         # print(x_ir.shape)\n",
        "#         ###############  image x_ir ([B, 49, 512])\n",
        "#\n",
        "#         # y_hat = self.pyramid_fuse(x_ir, x_face)\n",
        "#         # y_hat = self.se_block(y_hat)\n",
        "#         # y_feat = y_hat\n",
        "#\n",
        "#         # out = self.head(x_ir)\n",
        "#\n",
        "#         out = x_ir\n",
        "#         return out\n",
        "\n",
        "\n",
        "class eca_block(nn.Module):\n",
        "    def __init__(self, channel=196, b=1, gamma=2):\n",
        "        super(eca_block, self).__init__()\n",
        "        kernel_size = int(abs((math.log(channel, 2) + b) / gamma))\n",
        "        kernel_size = kernel_size if kernel_size % 2 else kernel_size + 1\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.conv = nn.Conv1d(1, 1, kernel_size=kernel_size, padding=(kernel_size - 1) // 2, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.avg_pool(x)\n",
        "        y = self.conv(y.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)\n",
        "        y = self.sigmoid(y)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "class SE_block(nn.Module):\n",
        "    def __init__(self, input_dim: int):\n",
        "        super().__init__()\n",
        "        self.linear1 = torch.nn.Linear(input_dim, input_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = torch.nn.Linear(input_dim, input_dim)\n",
        "        self.sigmod = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.linear1(x)\n",
        "        x1 = self.relu(x1)\n",
        "        x1 = self.linear2(x1)\n",
        "        x1 = self.sigmod(x1)\n",
        "        x = x * x1\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, img_size=14, patch_size=14, in_c=147, num_classes=7,\n",
        "                 embed_dim=768, depth=6, num_heads=8, mlp_ratio=4.0, qkv_bias=True,\n",
        "                 qk_scale=None, representation_size=None, distilled=False, drop_ratio=0.,\n",
        "                 attn_drop_ratio=0., drop_path_ratio=0., embed_layer=PatchEmbed, norm_layer=None,\n",
        "                 act_layer=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_size (int, tuple): input image size\n",
        "            patch_size (int, tuple): patch size\n",
        "            in_c (int): number of input channels\n",
        "            num_classes (int): number of classes for classification head\n",
        "            embed_dim (int): embedding dimension\n",
        "            depth (int): depth of transformer\n",
        "            num_heads (int): number of attention heads\n",
        "            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n",
        "            qkv_bias (bool): enable bias for qkv if True\n",
        "            qk_scale (float): override default qk scale of head_dim ** -0.5 if set\n",
        "            representation_size (Optional[int]): enable and set representation layer (pre-logits) to this value if set\n",
        "            distilled (bool): model includes a distillation token and head as in DeiT models\n",
        "            drop_ratio (float): dropout rate\n",
        "            attn_drop_ratio (float): attention dropout rate\n",
        "            drop_path_ratio (float): stochastic depth rate\n",
        "            embed_layer (nn.Module): patch embedding layer\n",
        "            norm_layer: (nn.Module): normalization layer\n",
        "        \"\"\"\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
        "        self.num_tokens = 2 if distilled else 1\n",
        "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
        "        act_layer = act_layer or nn.GELU\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, in_c + 1, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(p=drop_ratio)\n",
        "\n",
        "        self.se_block = SE_block(input_dim=embed_dim)\n",
        "\n",
        "\n",
        "        self.patch_embed = embed_layer(img_size=img_size, patch_size=patch_size, in_c=256, embed_dim=768)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        self.head = ClassificationHead(input_dim=embed_dim, target_dim=self.num_classes)\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.dist_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if distilled else None\n",
        "        # self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(p=drop_ratio)\n",
        "        # self.IR = IR()\n",
        "        self.eca_block = eca_block()\n",
        "\n",
        "\n",
        "        # self.ir_back = Backbone(50, 0.0, 'ir')\n",
        "        # ir_checkpoint = torch.load('./models/pretrain/ir50.pth', map_location=lambda storage, loc: storage)\n",
        "        # # ir_checkpoint = ir_checkpoint[\"model\"]\n",
        "        # self.ir_back = load_pretrained_weights(self.ir_back, ir_checkpoint)\n",
        "\n",
        "        self.CON1 = nn.Conv2d(256, 768, kernel_size=1, stride=1, bias=False)\n",
        "        self.IRLinear1 = nn.Linear(1024, 768)\n",
        "        self.IRLinear2 = nn.Linear(768, 512)\n",
        "        self.eca_block = eca_block()\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_ratio, depth)]  # stochastic depth decay rule\n",
        "        self.blocks = nn.Sequential(*[\n",
        "            Block(dim=embed_dim, in_chans=in_c, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias,\n",
        "                  qk_scale=qk_scale,\n",
        "                  drop_ratio=drop_ratio, attn_drop_ratio=attn_drop_ratio, drop_path_ratio=dpr[i],\n",
        "                  norm_layer=norm_layer, act_layer=act_layer)\n",
        "            for i in range(depth)\n",
        "        ])\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "        # Representation layer\n",
        "        if representation_size and not distilled:\n",
        "            self.has_logits = True\n",
        "            self.num_features = representation_size\n",
        "            self.pre_logits = nn.Sequential(OrderedDict([\n",
        "                (\"fc\", nn.Linear(embed_dim, representation_size)),\n",
        "                (\"act\", nn.Tanh())\n",
        "            ]))\n",
        "        else:\n",
        "            self.has_logits = False\n",
        "            self.pre_logits = nn.Identity()\n",
        "\n",
        "        # Classifier head(s)\n",
        "        # self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
        "        self.head_dist = None\n",
        "        if distilled:\n",
        "            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "        # Weight init\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "        if self.dist_token is not None:\n",
        "            nn.init.trunc_normal_(self.dist_token, std=0.02)\n",
        "\n",
        "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "        self.apply(_init_vit_weights)\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        # [B, C, H, W] -> [B, num_patches, embed_dim]\n",
        "        # x = self.patch_embed(x)  # [B, 196, 768]\n",
        "        # [1, 1, 768] -> [B, 1, 768]\n",
        "        # print(x.shape)\n",
        "\n",
        "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
        "        if self.dist_token is None:\n",
        "            x = torch.cat((cls_token, x), dim=1)  # [B, 197, 768]\n",
        "        else:\n",
        "            x = torch.cat((cls_token, self.dist_token.expand(x.shape[0], -1, -1), x), dim=1)\n",
        "        # print(x.shape)\n",
        "        x = self.pos_drop(x + self.pos_embed)\n",
        "        x = self.blocks(x)\n",
        "        x = self.norm(x)\n",
        "        if self.dist_token is None:\n",
        "            return self.pre_logits(x[:, 0])\n",
        "        else:\n",
        "            return x[:, 0], x[:, 1]\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # B = x.shape[0]\n",
        "        # print(x)\n",
        "        # x = self.eca_block(x)\n",
        "        # x = self.IR(x)\n",
        "        # x = eca_block(x)\n",
        "        # x = self.ir_back(x)\n",
        "        # print(x.shape)\n",
        "        # x = self.CON1(x)\n",
        "        # x = x.view(-1, 196, 768)\n",
        "        #\n",
        "        # # print(x.shape)\n",
        "        # # x = self.IRLinear1(x)\n",
        "        # # print(x)\n",
        "        # x_cls = torch.mean(x, 1).view(B, 1, -1)\n",
        "        # x = torch.cat((x_cls, x), dim=1)\n",
        "        # # print(x.shape)\n",
        "        # x = self.pos_drop(x + self.pos_embed)\n",
        "        # # print(x.shape)\n",
        "        # x = self.blocks(x)\n",
        "        # # print(x)\n",
        "        # x = self.norm(x)\n",
        "        # # print(x)\n",
        "        # # x1 = self.IRLinear2(x)\n",
        "        # x1 = x[:, 0, :]\n",
        "\n",
        "        # print(x1)\n",
        "        # print(x1.shape)\n",
        "\n",
        "        x = self.forward_features(x)\n",
        "        # # print(x.shape)\n",
        "        # if self.head_dist is not None:\n",
        "        #     x, x_dist = self.head(x[0]), self.head_dist(x[1])\n",
        "        #     if self.training and not torch.jit.is_scripting():\n",
        "        #         # during inference, return the average of both classifier predictions\n",
        "        #         return x, x_dist\n",
        "        #     else:\n",
        "        #         return (x + x_dist) / 2\n",
        "        # else:\n",
        "        # print(x.shape)\n",
        "        x = self.se_block(x)\n",
        "\n",
        "        x1 = self.head(x)\n",
        "\n",
        "        return x1\n",
        "\n",
        "\n",
        "def _init_vit_weights(m):\n",
        "    \"\"\"\n",
        "    ViT weight initialization\n",
        "    :param m: module\n",
        "    \"\"\"\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.trunc_normal_(m.weight, std=.01)\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "    elif isinstance(m, nn.Conv2d):\n",
        "        nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "    elif isinstance(m, nn.LayerNorm):\n",
        "        nn.init.zeros_(m.bias)\n",
        "        nn.init.ones_(m.weight)\n",
        "\n",
        "\n",
        "def vit_base_patch16_224(num_classes: int = 7):\n",
        "    \"\"\"\n",
        "    ViT-Base model (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).\n",
        "    ImageNet-1k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n",
        "    weights ported from official Google JAX impl:\n",
        "    链接: https://pan.baidu.com/s/1zqb08naP0RPqqfSXfkB2EA  密码: eu9f\n",
        "    \"\"\"\n",
        "    model = VisionTransformer(img_size=224,\n",
        "                              patch_size=16,\n",
        "                              embed_dim=768,\n",
        "                              depth=12,\n",
        "                              num_heads=12,\n",
        "                              representation_size=None,\n",
        "                              num_classes=num_classes)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def vit_base_patch16_224_in21k(num_classes: int = 21843, has_logits: bool = True):\n",
        "    \"\"\"\n",
        "    ViT-Base model (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).\n",
        "    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n",
        "    weights ported from official Google JAX impl:\n",
        "    https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_patch16_224_in21k-e5005f0a.pth\n",
        "    \"\"\"\n",
        "    model = VisionTransformer(img_size=224,\n",
        "                              patch_size=16,\n",
        "                              embed_dim=768,\n",
        "                              depth=12,\n",
        "                              num_heads=12,\n",
        "                              representation_size=768 if has_logits else None,\n",
        "                              num_classes=num_classes)\n",
        "    return model\n",
        "\n",
        "\n",
        "def vit_base_patch32_224(num_classes: int = 1000):\n",
        "    \"\"\"\n",
        "    ViT-Base model (ViT-B/32) from original paper (https://arxiv.org/abs/2010.11929).\n",
        "    ImageNet-1k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n",
        "    weights ported from official Google JAX impl:\n",
        "    链接: https://pan.baidu.com/s/1hCv0U8pQomwAtHBYc4hmZg  密码: s5hl\n",
        "    \"\"\"\n",
        "    model = VisionTransformer(img_size=224,\n",
        "                              patch_size=32,\n",
        "                              embed_dim=768,\n",
        "                              depth=12,\n",
        "                              num_heads=12,\n",
        "                              representation_size=None,\n",
        "                              num_classes=num_classes)\n",
        "    return model\n",
        "\n",
        "\n",
        "def vit_base_patch32_224_in21k(num_classes: int = 21843, has_logits: bool = True):\n",
        "    \"\"\"\n",
        "    ViT-Base model (ViT-B/32) from original paper (https://arxiv.org/abs/2010.11929).\n",
        "    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n",
        "    weights ported from official Google JAX impl:\n",
        "    https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_base_patch32_224_in21k-8db57226.pth\n",
        "    \"\"\"\n",
        "    model = VisionTransformer(img_size=224,\n",
        "                              patch_size=32,\n",
        "                              embed_dim=768,\n",
        "                              depth=12,\n",
        "                              num_heads=12,\n",
        "                              representation_size=768 if has_logits else None,\n",
        "                              num_classes=num_classes)\n",
        "    return model\n",
        "\n",
        "\n",
        "def vit_large_patch16_224(num_classes: int = 1000):\n",
        "    \"\"\"\n",
        "    ViT-Large model (ViT-L/16) from original paper (https://arxiv.org/abs/2010.11929).\n",
        "    ImageNet-1k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n",
        "    weights ported from official Google JAX impl:\n",
        "    链接: https://pan.baidu.com/s/1cxBgZJJ6qUWPSBNcE4TdRQ  密码: qqt8\n",
        "    \"\"\"\n",
        "    model = VisionTransformer(img_size=224,\n",
        "                              patch_size=16,\n",
        "                              embed_dim=1024,\n",
        "                              depth=24,\n",
        "                              num_heads=16,\n",
        "                              representation_size=None,\n",
        "                              num_classes=num_classes)\n",
        "    return model\n",
        "\n",
        "\n",
        "def vit_large_patch16_224_in21k(num_classes: int = 21843, has_logits: bool = True):\n",
        "    \"\"\"\n",
        "    ViT-Large model (ViT-L/16) from original paper (https://arxiv.org/abs/2010.11929).\n",
        "    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n",
        "    weights ported from official Google JAX impl:\n",
        "    https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_patch16_224_in21k-606da67d.pth\n",
        "    \"\"\"\n",
        "    model = VisionTransformer(img_size=224,\n",
        "                              patch_size=16,\n",
        "                              embed_dim=1024,\n",
        "                              depth=24,\n",
        "                              num_heads=16,\n",
        "                              representation_size=1024 if has_logits else None,\n",
        "                              num_classes=num_classes)\n",
        "    return model\n",
        "\n",
        "\n",
        "def vit_large_patch32_224_in21k(num_classes: int = 21843, has_logits: bool = True):\n",
        "    \"\"\"\n",
        "    ViT-Large model (ViT-L/32) from original paper (https://arxiv.org/abs/2010.11929).\n",
        "    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n",
        "    weights ported from official Google JAX impl:\n",
        "    https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx/jx_vit_large_patch32_224_in21k-9046d2e7.pth\n",
        "    \"\"\"\n",
        "    model = VisionTransformer(img_size=224,\n",
        "                              patch_size=32,\n",
        "                              embed_dim=1024,\n",
        "                              depth=24,\n",
        "                              num_heads=16,\n",
        "                              representation_size=1024 if has_logits else None,\n",
        "                              num_classes=num_classes)\n",
        "    return model\n",
        "\n",
        "\n",
        "def vit_huge_patch14_224_in21k(num_classes: int = 21843, has_logits: bool = True):\n",
        "    \"\"\"\n",
        "    ViT-Huge model (ViT-H/14) from original paper (https://arxiv.org/abs/2010.11929).\n",
        "    ImageNet-21k weights @ 224x224, source https://github.com/google-research/vision_transformer.\n",
        "    NOTE: converted weights not currently available, too large for github release hosting.\n",
        "    \"\"\"\n",
        "    model = VisionTransformer(img_size=224,\n",
        "                              patch_size=14,\n",
        "                              embed_dim=1280,\n",
        "                              depth=32,\n",
        "                              num_heads=16,\n",
        "                              representation_size=1280 if has_logits else None,\n",
        "                              num_classes=num_classes)\n",
        "    return model"
      ],
      "metadata": {
        "id": "dR5cVqS4tze_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### tools/sam.py\n",
        "- sam.py: sam씨가 만든 py파일일까요..흠 여기서 two step optimizer 쓰는데 아직 내용 파악 못했습니다.Poster같은 경우 /data_preprocessing에 넣어뒀더라구요."
      ],
      "metadata": {
        "id": "0MewdRPpt-cg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class SAM(torch.optim.Optimizer):\n",
        "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
        "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
        "\n",
        "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
        "        super(SAM, self).__init__(params, defaults)\n",
        "\n",
        "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
        "        self.param_groups = self.base_optimizer.param_groups\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def first_step(self, zero_grad=False):\n",
        "        grad_norm = self._grad_norm()\n",
        "        for group in self.param_groups:\n",
        "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
        "\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None: continue\n",
        "                self.state[p][\"old_p\"] = p.data.clone()\n",
        "                e_w = (torch.pow(p, 2) if group[\"adaptive\"] else 1.0) * p.grad * scale.to(p)\n",
        "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
        "\n",
        "        if zero_grad: self.zero_grad()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def second_step(self, zero_grad=False):\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None: continue\n",
        "                p.data = self.state[p][\"old_p\"]  # get back to \"w\" from \"w + e(w)\"\n",
        "\n",
        "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
        "\n",
        "        if zero_grad: self.zero_grad()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
        "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
        "\n",
        "        self.first_step(zero_grad=True)\n",
        "        closure()\n",
        "        self.second_step()\n",
        "\n",
        "    def _grad_norm(self):\n",
        "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
        "        norm = torch.norm(\n",
        "                    torch.stack([\n",
        "                        ((torch.abs(p) if group[\"adaptive\"] else 1.0) * p.grad).norm(p=2).to(shared_device)\n",
        "                        for group in self.param_groups for p in group[\"params\"]\n",
        "                        if p.grad is not None\n",
        "                    ]),\n",
        "                    p=2\n",
        "               )\n",
        "        return norm\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        super().load_state_dict(state_dict)\n",
        "        self.base_optimizer.param_groups = self.param_groups\n"
      ],
      "metadata": {
        "id": "B7M8uuwfuqGv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}